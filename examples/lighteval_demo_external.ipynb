{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightEval Framework Adapter - External Client Demo\n",
    "\n",
    "**Architecture Overview:**\n",
    "- üì¶ **Container**: Pre-built container running LightEval + EvalHub SDK adapter (exposes REST API)\n",
    "- üíª **This Notebook**: External client making HTTP requests to the container\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    HTTP/REST    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Jupyter Notebook  ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ      Container          ‚îÇ\n",
    "‚îÇ   (External Client) ‚îÇ                 ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
    "‚îÇ                     ‚îÇ ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ  ‚îÇ LightEval + Adapter ‚îÇ ‚îÇ\n",
    "‚îÇ - Make requests     ‚îÇ    JSON API     ‚îÇ  ‚îÇ   (Port 8000)       ‚îÇ ‚îÇ\n",
    "‚îÇ - Display results   ‚îÇ                 ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
    "‚îÇ - Test endpoints    ‚îÇ                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "## What This Demo Shows\n",
    "\n",
    "1. **Connecting** to a pre-built LightEval adapter container\n",
    "2. **Making HTTP requests** from this notebook to test all API endpoints\n",
    "3. **Submitting evaluation jobs** and monitoring their progress\n",
    "4. **Retrieving results** and handling different response types\n",
    "5. **Testing error handling** and edge cases\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Pre-built container**: Container must be already built and running\n",
    "- **Python with `requests`**: For HTTP client functionality\n",
    "- **Network access**: For communicating with container API\n",
    "- **Container running on port 8000**: Default port configuration\n",
    "\n",
    "### Building and Running the Container\n",
    "\n",
    "If you haven't built the container yet, use these commands:\n",
    "\n",
    "```bash\n",
    "# Build the container (from evalhub-sdk root directory)\n",
    "podman build -t evalhub/lighteval-adapter:latest -f examples/lighteval_adapter/Dockerfile .\n",
    "\n",
    "# Run the container\n",
    "podman run -d \\\n",
    "  --name lighteval-adapter \\\n",
    "  -p 8000:8000 \\\n",
    "  --health-cmd='curl -f http://localhost:8000/api/v1/health || exit 1' \\\n",
    "  --health-interval=30s \\\n",
    "  --health-timeout=10s \\\n",
    "  --health-start-period=30s \\\n",
    "  --health-retries=3 \\\n",
    "  evalhub/lighteval-adapter:latest\n",
    "\n",
    "# Check container status\n",
    "podman ps\n",
    "```\n",
    "\n",
    "**Note**: This notebook assumes the container is already built and running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:58:31.035408606Z",
     "start_time": "2025-12-15T09:58:30.559120064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External client notebook started\n",
      "Connecting to running containerized LightEval adapter\n",
      "All communication via HTTP/REST API\n",
      "Make sure container is running on port 8000!\n"
     ]
    }
   ],
   "source": [
    "# üìã IMPORTANT: This notebook runs OUTSIDE the container!\n",
    "#\n",
    "# This notebook makes HTTP requests to a running LightEval adapter container.\n",
    "# The container contains:\n",
    "#   - LightEval evaluation framework\n",
    "#   - EvalHub SDK adapter wrapper\n",
    "#   - REST API server (FastAPI)\n",
    "#\n",
    "# This notebook acts as an external client testing the API.\n",
    "# Make sure the container is already built and running before executing this notebook.\n",
    "\n",
    "import json\n",
    "import time\n",
    "from typing import Any\n",
    "\n",
    "import requests\n",
    "\n",
    "print(\"External client notebook started\")\n",
    "print(\"Connecting to running containerized LightEval adapter\")\n",
    "print(\"All communication via HTTP/REST API\")\n",
    "print(\"Make sure container is running on port 8000!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:58:35.130087175Z",
     "start_time": "2025-12-15T09:58:35.048350553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container: lighteval-adapter\n",
      "Image: evalhub/lighteval-adapter:latest\n",
      "API Base URL: http://localhost:8000/api/v1\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CONTAINER_NAME = \"lighteval-adapter\"\n",
    "IMAGE_NAME = \"evalhub/lighteval-adapter\"\n",
    "IMAGE_TAG = \"latest\"\n",
    "FULL_IMAGE_NAME = f\"{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "CONTAINER_PORT = 8000\n",
    "HOST_PORT = 8000\n",
    "BASE_URL = f\"http://localhost:{HOST_PORT}\"\n",
    "API_BASE = f\"{BASE_URL}/api/v1\"\n",
    "\n",
    "print(f\"Container: {CONTAINER_NAME}\")\n",
    "print(f\"Image: {FULL_IMAGE_NAME}\")\n",
    "print(f\"API Base URL: {API_BASE}\")\n",
    "\n",
    "# Helper function for making HTTP requests\n",
    "def make_request(method: str, endpoint: str, data: dict[str, Any] | None = None) -> dict[str, Any]:\n",
    "    \"\"\"Make an HTTP API request to the containerized adapter and return the JSON response.\"\"\"\n",
    "    url = f\"{API_BASE}{endpoint}\"\n",
    "    print(f\"\\nüåê External HTTP Request: {method.upper()} {url}\")\n",
    "\n",
    "    try:\n",
    "        if method.lower() == 'get':\n",
    "            response = requests.get(url, timeout=10)\n",
    "        elif method.lower() == 'post':\n",
    "            response = requests.post(url, json=data, timeout=10)\n",
    "        elif method.lower() == 'delete':\n",
    "            response = requests.delete(url, timeout=10)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported method: {method}\")\n",
    "\n",
    "        print(f\"üì° Response Status: {response.status_code}\")\n",
    "\n",
    "        if response.headers.get('content-type', '').startswith('application/json'):\n",
    "            result = response.json()\n",
    "            print(f\"üìã JSON Response: {json.dumps(result, indent=2)}\")\n",
    "            return result\n",
    "        else:\n",
    "            print(f\"üìÑ Text Response: {response.text}\")\n",
    "            return {\"text\": response.text}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Request Error: {e}\")\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:58:38.812718363Z",
     "start_time": "2025-12-15T09:58:38.741704780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To build and run the container, execute these commands in your terminal:\n",
      "1. Build: podman build -t evalhub/lighteval-adapter:latest -f examples/lighteval_adapter/Dockerfile .\n",
      "2. Run: podman run -d --name lighteval-adapter -p 8000:8000 evalhub/lighteval-adapter:latest\n",
      "3. Then continue with this notebook\n"
     ]
    }
   ],
   "source": [
    "# Container build commands (run these manually in terminal)\n",
    "build_command = f\"podman build -t {FULL_IMAGE_NAME} -f examples/lighteval_adapter/Dockerfile .\"\n",
    "run_command = f\"podman run -d --name {CONTAINER_NAME} -p {HOST_PORT}:{CONTAINER_PORT} {FULL_IMAGE_NAME}\"\n",
    "\n",
    "print(\"To build and run the container, execute these commands in your terminal:\")\n",
    "print(f\"1. Build: {build_command}\")\n",
    "print(f\"2. Run: {run_command}\")\n",
    "print(\"3. Then continue with this notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:59:42.392524658Z",
     "start_time": "2025-12-15T09:59:42.316314162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for container to be ready...\n",
      "‚úÖ Container is ready!\n"
     ]
    }
   ],
   "source": [
    "# Wait for the container to be ready\n",
    "print(\"Waiting for container to be ready...\")\n",
    "max_retries = 30\n",
    "retry_count = 0\n",
    "\n",
    "while retry_count < max_retries:\n",
    "    try:\n",
    "        response = requests.get(f\"{API_BASE}/health\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ Container is ready!\")\n",
    "            break\n",
    "    except requests.exceptions.RequestException:\n",
    "        pass\n",
    "\n",
    "    retry_count += 1\n",
    "    print(f\"Retry {retry_count}/{max_retries}...\")\n",
    "    time.sleep(2)\n",
    "\n",
    "if retry_count >= max_retries:\n",
    "    print(\"‚ùå Container failed to become ready\")\n",
    "    print(\"Tip: Check container logs with: podman logs lighteval-adapter\")\n",
    "    raise RuntimeError(\"Container not ready after maximum retries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:59:46.035168664Z",
     "start_time": "2025-12-15T09:59:45.952814570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Container found: lighteval-adapter Up 10 seconds\n"
     ]
    }
   ],
   "source": [
    "# Check if the container is running\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def check_container_status():\n",
    "    \"\"\"Check if the lighteval-adapter container is running.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"podman\", \"ps\", \"--filter\", \"name=lighteval-adapter\", \"--format\", \"{{.Names}} {{.Status}}\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        if \"lighteval-adapter\" in result.stdout:\n",
    "            print(f\"‚úÖ Container found: {result.stdout.strip()}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Container 'lighteval-adapter' not found in running containers\")\n",
    "            return False\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Failed to check container status: {e}\")\n",
    "        return False\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Podman not found. Make sure podman is installed and in PATH.\")\n",
    "        return False\n",
    "\n",
    "# Check container status\n",
    "container_running = check_container_status()\n",
    "\n",
    "if not container_running:\n",
    "    print(\"\\nTo start the container, run these commands:\")\n",
    "    print(\"podman run -d \\\\\")\n",
    "    print(\"  --name lighteval-adapter \\\\\")\n",
    "    print(\"  -p 8000:8000 \\\\\")\n",
    "    print(\"  --health-cmd='curl -f http://localhost:8000/api/v1/health || exit 1' \\\\\")\n",
    "    print(\"  --health-interval=30s \\\\\")\n",
    "    print(\"  evalhub/lighteval-adapter:latest\")\n",
    "    print(\"\\nThen re-run this notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:59:49.328477187Z",
     "start_time": "2025-12-15T09:59:49.264293310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for API to be fully ready...\n",
      "‚úÖ API is ready!\n",
      "üöÄ Ready to test LightEval adapter endpoints!\n"
     ]
    }
   ],
   "source": [
    "# Wait for API to be fully ready (optional)\n",
    "print(\"Waiting for API to be fully ready...\")\n",
    "max_retries = 10\n",
    "retry_count = 0\n",
    "\n",
    "while retry_count < max_retries:\n",
    "    try:\n",
    "        response = requests.get(f\"{API_BASE}/health\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ API is ready!\")\n",
    "            break\n",
    "    except requests.exceptions.RequestException:\n",
    "        pass\n",
    "\n",
    "    retry_count += 1\n",
    "    print(f\"Retry {retry_count}/{max_retries}...\")\n",
    "    time.sleep(1)\n",
    "\n",
    "if retry_count >= max_retries:\n",
    "    print(\"‚ùå API not responding after retries\")\n",
    "    print(\"Check container logs: podman logs lighteval-adapter\")\n",
    "else:\n",
    "    print(\"üöÄ Ready to test LightEval adapter endpoints!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Test the Adapter Endpoints via HTTP\n",
    "\n",
    "üåê **Now we'll test the running LightEval adapter from this external notebook**\n",
    "\n",
    "The container is running the LightEval framework + EvalHub adapter on port 8000.  \n",
    "This notebook acts as an external client making HTTP requests to test all endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:59:53.714279612Z",
     "start_time": "2025-12-15T09:59:53.628994998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåê External HTTP Request: GET http://localhost:8000/api/v1/info\n",
      "üì° Response Status: 200\n",
      "üìã JSON Response: {\n",
      "  \"framework_id\": \"lighteval\",\n",
      "  \"name\": \"LightEval Framework Adapter\",\n",
      "  \"version\": \"1.0.0\",\n",
      "  \"description\": \"LightEval is a lightweight evaluation framework for language models\",\n",
      "  \"supported_benchmarks\": [\n",
      "    {\n",
      "      \"benchmark_id\": \"hellaswag\",\n",
      "      \"name\": \"HellaSwag\",\n",
      "      \"description\": \"LightEval task: HellaSwag\",\n",
      "      \"category\": \"Commonsense reasoning\",\n",
      "      \"tags\": [],\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"dataset_size\": null,\n",
      "      \"supports_few_shot\": true,\n",
      "      \"default_few_shot\": null,\n",
      "      \"custom_config_schema\": null\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"arc:easy\",\n",
      "      \"name\": \"ARC Easy\",\n",
      "      \"description\": \"LightEval task: ARC Easy\",\n",
      "      \"category\": \"Scientific reasoning\",\n",
      "      \"tags\": [],\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"dataset_size\": null,\n",
      "      \"supports_few_shot\": true,\n",
      "      \"default_few_shot\": null,\n",
      "      \"custom_config_schema\": null\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"arc:challenge\",\n",
      "      \"name\": \"ARC Challenge\",\n",
      "      \"description\": \"LightEval task: ARC Challenge\",\n",
      "      \"category\": \"Scientific reasoning\",\n",
      "      \"tags\": [],\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"dataset_size\": null,\n",
      "      \"supports_few_shot\": true,\n",
      "      \"default_few_shot\": null,\n",
      "      \"custom_config_schema\": null\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"piqa\",\n",
      "      \"name\": \"PIQA\",\n",
      "      \"description\": \"LightEval task: PIQA\",\n",
      "      \"category\": \"Physical commonsense\",\n",
      "      \"tags\": [],\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"dataset_size\": null,\n",
      "      \"supports_few_shot\": true,\n",
      "      \"default_few_shot\": null,\n",
      "      \"custom_config_schema\": null\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"winogrande\",\n",
      "      \"name\": \"WinoGrande\",\n",
      "      \"description\": \"LightEval task: WinoGrande\",\n",
      "      \"category\": \"Commonsense reasoning\",\n",
      "      \"tags\": [],\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"dataset_size\": null,\n",
      "      \"supports_few_shot\": true,\n",
      "      \"default_few_shot\": null,\n",
      "      \"custom_config_schema\": null\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"truthfulqa:mc\",\n",
      "      \"name\": \"TruthfulQA\",\n",
      "      \"description\": \"LightEval task: TruthfulQA\",\n",
      "      \"category\": \"Truthfulness\",\n",
      "      \"tags\": [],\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"dataset_size\": null,\n",
      "      \"supports_few_shot\": true,\n",
      "      \"default_few_shot\": null,\n",
      "      \"custom_config_schema\": null\n",
      "    }\n",
      "  ],\n",
      "  \"supported_model_types\": [\n",
      "    \"transformers\",\n",
      "    \"vllm\",\n",
      "    \"openai\",\n",
      "    \"anthropic\",\n",
      "    \"endpoint\"\n",
      "  ],\n",
      "  \"default_model_config\": {},\n",
      "  \"config_schema\": null,\n",
      "  \"author\": null,\n",
      "  \"contact\": null,\n",
      "  \"documentation_url\": null,\n",
      "  \"repository_url\": null\n",
      "}\n",
      "‚úÖ Framework: LightEval Framework Adapter\n",
      "   Version: 1.0.0\n",
      "   Supported benchmarks: 6\n",
      "   Supported models: transformers, vllm, openai, anthropic, endpoint\n"
     ]
    }
   ],
   "source": [
    "# Get framework information\n",
    "info_response = make_request('GET', '/info')\n",
    "\n",
    "if 'framework_id' in info_response:\n",
    "    print(f\"‚úÖ Framework: {info_response['name']}\")\n",
    "    print(f\"   Version: {info_response['version']}\")\n",
    "    print(f\"   Supported benchmarks: {len(info_response.get('supported_benchmarks', []))}\")\n",
    "    print(f\"   Supported models: {', '.join(info_response.get('supported_model_types', []))}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to get framework info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:00:00.028791591Z",
     "start_time": "2025-12-15T09:59:59.952908350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully connected to LightEval adapter API!\n",
      "   API Base URL: http://localhost:8000/api/v1\n",
      "   Health check: 200\n"
     ]
    }
   ],
   "source": [
    "# Test initial connection to the container API\n",
    "try:\n",
    "    response = requests.get(f\"{API_BASE}/health\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        print(\"‚úÖ Successfully connected to LightEval adapter API!\")\n",
    "        print(f\"   API Base URL: {API_BASE}\")\n",
    "        print(f\"   Health check: {response.status_code}\")\n",
    "    else:\n",
    "        print(f\"API responded with status code: {response.status_code}\")\n",
    "        print(\"Container may be starting up. Wait a moment and try again.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"‚ùå Failed to connect to API at {API_BASE}\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nMake sure the container is running on port 8000.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:00:03.889192955Z",
     "start_time": "2025-12-15T10:00:03.750241381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåê External HTTP Request: GET http://localhost:8000/api/v1/benchmarks\n",
      "üì° Response Status: 200\n",
      "üìã JSON Response: [\n",
      "  {\n",
      "    \"benchmark_id\": \"hellaswag\",\n",
      "    \"name\": \"HellaSwag\",\n",
      "    \"description\": \"LightEval task: HellaSwag\",\n",
      "    \"category\": \"Commonsense reasoning\",\n",
      "    \"tags\": [],\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"dataset_size\": null,\n",
      "    \"supports_few_shot\": true,\n",
      "    \"default_few_shot\": null,\n",
      "    \"custom_config_schema\": null\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"arc:easy\",\n",
      "    \"name\": \"ARC Easy\",\n",
      "    \"description\": \"LightEval task: ARC Easy\",\n",
      "    \"category\": \"Scientific reasoning\",\n",
      "    \"tags\": [],\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"dataset_size\": null,\n",
      "    \"supports_few_shot\": true,\n",
      "    \"default_few_shot\": null,\n",
      "    \"custom_config_schema\": null\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"arc:challenge\",\n",
      "    \"name\": \"ARC Challenge\",\n",
      "    \"description\": \"LightEval task: ARC Challenge\",\n",
      "    \"category\": \"Scientific reasoning\",\n",
      "    \"tags\": [],\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"dataset_size\": null,\n",
      "    \"supports_few_shot\": true,\n",
      "    \"default_few_shot\": null,\n",
      "    \"custom_config_schema\": null\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"piqa\",\n",
      "    \"name\": \"PIQA\",\n",
      "    \"description\": \"LightEval task: PIQA\",\n",
      "    \"category\": \"Physical commonsense\",\n",
      "    \"tags\": [],\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"dataset_size\": null,\n",
      "    \"supports_few_shot\": true,\n",
      "    \"default_few_shot\": null,\n",
      "    \"custom_config_schema\": null\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"winogrande\",\n",
      "    \"name\": \"WinoGrande\",\n",
      "    \"description\": \"LightEval task: WinoGrande\",\n",
      "    \"category\": \"Commonsense reasoning\",\n",
      "    \"tags\": [],\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"dataset_size\": null,\n",
      "    \"supports_few_shot\": true,\n",
      "    \"default_few_shot\": null,\n",
      "    \"custom_config_schema\": null\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"truthfulqa:mc\",\n",
      "    \"name\": \"TruthfulQA\",\n",
      "    \"description\": \"LightEval task: TruthfulQA\",\n",
      "    \"category\": \"Truthfulness\",\n",
      "    \"tags\": [],\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"dataset_size\": null,\n",
      "    \"supports_few_shot\": true,\n",
      "    \"default_few_shot\": null,\n",
      "    \"custom_config_schema\": null\n",
      "  }\n",
      "]\n",
      "‚úÖ Found 6 benchmarks:\n",
      "   1. HellaSwag (hellaswag)\n",
      "      Category: Commonsense reasoning\n",
      "      Metrics: accuracy\n",
      "   2. ARC Easy (arc:easy)\n",
      "      Category: Scientific reasoning\n",
      "      Metrics: accuracy\n",
      "   3. ARC Challenge (arc:challenge)\n",
      "      Category: Scientific reasoning\n",
      "      Metrics: accuracy\n",
      "   4. PIQA (piqa)\n",
      "      Category: Physical commonsense\n",
      "      Metrics: accuracy\n",
      "   5. WinoGrande (winogrande)\n",
      "      Category: Commonsense reasoning\n",
      "      Metrics: accuracy\n",
      "   ... and 1 more\n"
     ]
    }
   ],
   "source": [
    "# List available benchmarks\n",
    "benchmarks_response = make_request('GET', '/benchmarks')\n",
    "\n",
    "if isinstance(benchmarks_response, list) and benchmarks_response:\n",
    "    print(f\"‚úÖ Found {len(benchmarks_response)} benchmarks:\")\n",
    "    for i, benchmark in enumerate(benchmarks_response[:5]):  # Show first 5\n",
    "        print(f\"   {i+1}. {benchmark['name']} ({benchmark['benchmark_id']})\")\n",
    "        print(f\"      Category: {benchmark.get('category', 'N/A')}\")\n",
    "        print(f\"      Metrics: {', '.join(benchmark.get('metrics', []))}\")\n",
    "\n",
    "    if len(benchmarks_response) > 5:\n",
    "        print(f\"   ... and {len(benchmarks_response) - 5} more\")\n",
    "else:\n",
    "    print(\"‚ùå No benchmarks found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:00:19.989793528Z",
     "start_time": "2025-12-15T10:00:19.925061550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåê External HTTP Request: GET http://localhost:8000/api/v1/health\n",
      "üì° Response Status: 200\n",
      "üìã JSON Response: {\n",
      "  \"status\": \"healthy\",\n",
      "  \"framework_id\": \"lighteval\",\n",
      "  \"version\": \"1.0.0\",\n",
      "  \"dependencies\": {\n",
      "    \"lighteval\": {\n",
      "      \"status\": \"available\"\n",
      "    }\n",
      "  },\n",
      "  \"memory_usage\": null,\n",
      "  \"gpu_usage\": null,\n",
      "  \"uptime_seconds\": 3600.0,\n",
      "  \"last_evaluation_time\": null,\n",
      "  \"metadata\": {}\n",
      "}\n",
      "‚úÖ Adapter is healthy!\n"
     ]
    }
   ],
   "source": [
    "# Test health endpoint\n",
    "health_response = make_request('GET', '/health')\n",
    "\n",
    "if health_response.get('status') == 'healthy':\n",
    "    print(\"‚úÖ Adapter is healthy!\")\n",
    "else:\n",
    "    print(\"‚ùå Adapter health check failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framework Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:00:21.982510632Z",
     "start_time": "2025-12-15T10:00:21.912597023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåê External HTTP Request: GET http://localhost:8000/api/v1/benchmarks/hellaswag\n",
      "üì° Response Status: 200\n",
      "üìã JSON Response: {\n",
      "  \"benchmark_id\": \"hellaswag\",\n",
      "  \"name\": \"HellaSwag\",\n",
      "  \"description\": \"LightEval task: HellaSwag\",\n",
      "  \"category\": \"Commonsense reasoning\",\n",
      "  \"tags\": [],\n",
      "  \"metrics\": [\n",
      "    \"accuracy\"\n",
      "  ],\n",
      "  \"dataset_size\": null,\n",
      "  \"supports_few_shot\": true,\n",
      "  \"default_few_shot\": null,\n",
      "  \"custom_config_schema\": null\n",
      "}\n",
      "‚úÖ Benchmark details for 'hellaswag':\n",
      "   Name: HellaSwag\n",
      "   Description: LightEval task: HellaSwag\n",
      "   Category: Commonsense reasoning\n",
      "   Metrics: accuracy\n"
     ]
    }
   ],
   "source": [
    "# Get details for a specific benchmark\n",
    "if isinstance(benchmarks_response, list) and benchmarks_response:\n",
    "    test_benchmark_id = benchmarks_response[0]['benchmark_id']\n",
    "    benchmark_detail = make_request('GET', f'/benchmarks/{test_benchmark_id}')\n",
    "\n",
    "    if 'benchmark_id' in benchmark_detail:\n",
    "        print(f\"‚úÖ Benchmark details for '{test_benchmark_id}':\")\n",
    "        print(f\"   Name: {benchmark_detail['name']}\")\n",
    "        print(f\"   Description: {benchmark_detail.get('description', 'N/A')}\")\n",
    "        print(f\"   Category: {benchmark_detail.get('category', 'N/A')}\")\n",
    "        print(f\"   Metrics: {', '.join(benchmark_detail.get('metrics', []))}\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to get benchmark details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Submit Evaluation Jobs\n",
    "\n",
    "Now we'll submit some evaluation jobs to test the adapter functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:00:23.980778062Z",
     "start_time": "2025-12-15T10:00:23.916819899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting evaluation for benchmark: hellaswag\n",
      "\n",
      "üåê External HTTP Request: POST http://localhost:8000/api/v1/evaluations\n",
      "üì° Response Status: 200\n",
      "üìã JSON Response: {\n",
      "  \"job_id\": \"58047dd4-711c-4a95-9bec-e366de422fea\",\n",
      "  \"status\": \"pending\",\n",
      "  \"evaluation_status\": null,\n",
      "  \"request\": {\n",
      "    \"benchmark_id\": \"hellaswag\",\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt2\",\n",
      "      \"provider\": \"huggingface\",\n",
      "      \"parameters\": {\n",
      "        \"temperature\": 0.1,\n",
      "        \"max_tokens\": 100\n",
      "      },\n",
      "      \"device\": null,\n",
      "      \"batch_size\": null\n",
      "    },\n",
      "    \"num_examples\": 10,\n",
      "    \"num_few_shot\": null,\n",
      "    \"random_seed\": 42,\n",
      "    \"benchmark_config\": {},\n",
      "    \"experiment_name\": \"demo_evaluation\",\n",
      "    \"tags\": {},\n",
      "    \"priority\": 0\n",
      "  },\n",
      "  \"submitted_at\": \"2025-12-15T10:00:23.920291Z\",\n",
      "  \"started_at\": null,\n",
      "  \"completed_at\": null,\n",
      "  \"progress\": null,\n",
      "  \"current_step\": null,\n",
      "  \"total_steps\": null,\n",
      "  \"completed_steps\": null,\n",
      "  \"error_message\": null,\n",
      "  \"error_details\": null,\n",
      "  \"estimated_duration\": null,\n",
      "  \"actual_duration\": null\n",
      "}\n",
      "‚úÖ Job submitted successfully!\n",
      "   Job ID: 58047dd4-711c-4a95-9bec-e366de422fea\n",
      "   Status: pending\n",
      "   Submitted at: 2025-12-15T10:00:23.920291Z\n"
     ]
    }
   ],
   "source": [
    "# Submit an evaluation job\n",
    "if isinstance(benchmarks_response, list) and benchmarks_response:\n",
    "    test_benchmark_id = benchmarks_response[0]['benchmark_id']\n",
    "\n",
    "    evaluation_request = {\n",
    "        \"benchmark_id\": test_benchmark_id,\n",
    "        \"model\": {\n",
    "            \"name\": \"gpt2\",\n",
    "            \"provider\": \"huggingface\",\n",
    "            \"parameters\": {\n",
    "                \"temperature\": 0.1,\n",
    "                \"max_tokens\": 100\n",
    "            }\n",
    "        },\n",
    "        \"num_examples\": 10,\n",
    "        \"experiment_name\": \"demo_evaluation\"\n",
    "    }\n",
    "\n",
    "    print(f\"Submitting evaluation for benchmark: {test_benchmark_id}\")\n",
    "    job_response = make_request('POST', '/evaluations', evaluation_request)\n",
    "\n",
    "    if 'job_id' in job_response:\n",
    "        job_id = job_response['job_id']\n",
    "        print(\"‚úÖ Job submitted successfully!\")\n",
    "        print(f\"   Job ID: {job_id}\")\n",
    "        print(f\"   Status: {job_response['status']}\")\n",
    "        print(f\"   Submitted at: {job_response['submitted_at']}\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to submit evaluation job\")\n",
    "        job_id = None\n",
    "else:\n",
    "    print(\"‚ùå No benchmarks available for testing\")\n",
    "    job_id = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor Job Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:00:30.421386325Z",
     "start_time": "2025-12-15T10:00:25.409358654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring job 58047dd4-711c-4a95-9bec-e366de422fea...\n",
      "\n",
      "üåê External HTTP Request: GET http://localhost:8000/api/v1/evaluations/58047dd4-711c-4a95-9bec-e366de422fea\n",
      "üì° Response Status: 200\n",
      "üìã JSON Response: {\n",
      "  \"job_id\": \"58047dd4-711c-4a95-9bec-e366de422fea\",\n",
      "  \"status\": \"running\",\n",
      "  \"evaluation_status\": null,\n",
      "  \"request\": {\n",
      "    \"benchmark_id\": \"hellaswag\",\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt2\",\n",
      "      \"provider\": \"huggingface\",\n",
      "      \"parameters\": {\n",
      "        \"temperature\": 0.1,\n",
      "        \"max_tokens\": 100\n",
      "      },\n",
      "      \"device\": null,\n",
      "      \"batch_size\": null\n",
      "    },\n",
      "    \"num_examples\": 10,\n",
      "    \"num_few_shot\": null,\n",
      "    \"random_seed\": 42,\n",
      "    \"benchmark_config\": {},\n",
      "    \"experiment_name\": \"demo_evaluation\",\n",
      "    \"tags\": {},\n",
      "    \"priority\": 0\n",
      "  },\n",
      "  \"submitted_at\": \"2025-12-15T10:00:23.920291Z\",\n",
      "  \"started_at\": \"2025-12-15T10:00:23.920557Z\",\n",
      "  \"completed_at\": null,\n",
      "  \"progress\": 0.3,\n",
      "  \"current_step\": null,\n",
      "  \"total_steps\": null,\n",
      "  \"completed_steps\": null,\n",
      "  \"error_message\": null,\n",
      "  \"error_details\": null,\n",
      "  \"estimated_duration\": null,\n",
      "  \"actual_duration\": null\n",
      "}\n",
      "Status: running, Progress: 30.0%\n",
      "\n",
      "üåê External HTTP Request: GET http://localhost:8000/api/v1/evaluations/58047dd4-711c-4a95-9bec-e366de422fea\n",
      "üì° Response Status: 200\n",
      "üìã JSON Response: {\n",
      "  \"job_id\": \"58047dd4-711c-4a95-9bec-e366de422fea\",\n",
      "  \"status\": \"completed\",\n",
      "  \"evaluation_status\": null,\n",
      "  \"request\": {\n",
      "    \"benchmark_id\": \"hellaswag\",\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt2\",\n",
      "      \"provider\": \"huggingface\",\n",
      "      \"parameters\": {\n",
      "        \"temperature\": 0.1,\n",
      "        \"max_tokens\": 100\n",
      "      },\n",
      "      \"device\": null,\n",
      "      \"batch_size\": null\n",
      "    },\n",
      "    \"num_examples\": 10,\n",
      "    \"num_few_shot\": null,\n",
      "    \"random_seed\": 42,\n",
      "    \"benchmark_config\": {},\n",
      "    \"experiment_name\": \"demo_evaluation\",\n",
      "    \"tags\": {},\n",
      "    \"priority\": 0\n",
      "  },\n",
      "  \"submitted_at\": \"2025-12-15T10:00:23.920291Z\",\n",
      "  \"started_at\": \"2025-12-15T10:00:23.920557Z\",\n",
      "  \"completed_at\": \"2025-12-15T10:00:25.949690Z\",\n",
      "  \"progress\": 1.0,\n",
      "  \"current_step\": null,\n",
      "  \"total_steps\": null,\n",
      "  \"completed_steps\": null,\n",
      "  \"error_message\": null,\n",
      "  \"error_details\": null,\n",
      "  \"estimated_duration\": null,\n",
      "  \"actual_duration\": null\n",
      "}\n",
      "Status: completed, Progress: 100.0%\n",
      "‚úÖ Job completed successfully!\n",
      "   Completed at: 2025-12-15T10:00:25.949690Z\n"
     ]
    }
   ],
   "source": [
    "# Monitor job progress\n",
    "if job_id:\n",
    "    print(f\"Monitoring job {job_id}...\")\n",
    "\n",
    "    max_wait_time = 300  # 5 minutes\n",
    "    start_time = time.time()\n",
    "\n",
    "    while time.time() - start_time < max_wait_time:\n",
    "        status_response = make_request('GET', f'/evaluations/{job_id}')\n",
    "\n",
    "        if 'status' in status_response:\n",
    "            status = status_response['status']\n",
    "            progress = status_response.get('progress', 0)\n",
    "\n",
    "            print(f\"Status: {status}, Progress: {progress*100 if progress else 0:.1f}%\")\n",
    "\n",
    "            if status in ['completed', 'failed', 'cancelled']:\n",
    "                if status == 'completed':\n",
    "                    print(\"‚úÖ Job completed successfully!\")\n",
    "                    print(f\"   Completed at: {status_response.get('completed_at')}\")\n",
    "                elif status == 'failed':\n",
    "                    print(\"‚ùå Job failed!\")\n",
    "                    print(f\"   Error: {status_response.get('error_message', 'Unknown error')}\")\n",
    "                else:\n",
    "                    print(\"‚è∏Ô∏è Job was cancelled\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"‚ùå Failed to get job status\")\n",
    "            break\n",
    "\n",
    "        time.sleep(5)  # Wait 5 seconds before checking again\n",
    "\n",
    "    else:\n",
    "        print(\"‚è∞ Job monitoring timed out\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No job to monitor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Submit Evaluation Jobs\n",
    "\n",
    "Now we'll submit some evaluation jobs to test the adapter functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:00:30.435326225Z",
     "start_time": "2025-12-15T10:00:30.422354007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåê External HTTP Request: GET http://localhost:8000/api/v1/evaluations/58047dd4-711c-4a95-9bec-e366de422fea/results\n",
      "üì° Response Status: 200\n",
      "üìã JSON Response: {\n",
      "  \"job_id\": \"58047dd4-711c-4a95-9bec-e366de422fea\",\n",
      "  \"benchmark_id\": \"hellaswag\",\n",
      "  \"model_name\": \"gpt2\",\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"metric_name\": \"accuracy\",\n",
      "      \"metric_value\": 0.81,\n",
      "      \"metric_type\": \"float\",\n",
      "      \"confidence_interval\": null,\n",
      "      \"num_samples\": 10,\n",
      "      \"metadata\": {}\n",
      "    }\n",
      "  ],\n",
      "  \"overall_score\": 0.81,\n",
      "  \"num_examples_evaluated\": 10,\n",
      "  \"evaluation_metadata\": {},\n",
      "  \"completed_at\": \"2025-12-15T10:00:25.949669Z\",\n",
      "  \"duration_seconds\": 120.0\n",
      "}\n",
      "‚úÖ Evaluation results for job 58047dd4-711c-4a95-9bec-e366de422fea:\n",
      "   Benchmark: hellaswag\n",
      "   Model: gpt2\n",
      "   Overall Score: 0.81\n",
      "   Examples Evaluated: 10\n",
      "   Duration: 120.0 seconds\n",
      "\n",
      "   Detailed Results:\n",
      "     accuracy: 0.81 (float)\n",
      "       Samples: 10\n"
     ]
    }
   ],
   "source": [
    "# Get evaluation results\n",
    "if job_id:\n",
    "    results_response = make_request('GET', f'/evaluations/{job_id}/results')\n",
    "\n",
    "    if 'results' in results_response:\n",
    "        print(f\"‚úÖ Evaluation results for job {job_id}:\")\n",
    "        print(f\"   Benchmark: {results_response['benchmark_id']}\")\n",
    "        print(f\"   Model: {results_response['model_name']}\")\n",
    "        print(f\"   Overall Score: {results_response.get('overall_score', 'N/A')}\")\n",
    "        print(f\"   Examples Evaluated: {results_response.get('num_examples_evaluated', 'N/A')}\")\n",
    "        print(f\"   Duration: {results_response.get('duration_seconds', 'N/A')} seconds\")\n",
    "\n",
    "        print(\"\\n   Detailed Results:\")\n",
    "        for result in results_response['results']:\n",
    "            metric_name = result['metric_name']\n",
    "            metric_value = result['metric_value']\n",
    "            metric_type = result.get('metric_type', 'unknown')\n",
    "            num_samples = result.get('num_samples', 'N/A')\n",
    "\n",
    "            print(f\"     {metric_name}: {metric_value} ({metric_type})\")\n",
    "            if num_samples != 'N/A':\n",
    "                print(f\"       Samples: {num_samples}\")\n",
    "    else:\n",
    "        print(\"‚ùå No results available yet or job not completed\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No job ID to get results for\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test Multiple Evaluations\n",
    "\n",
    "Let's submit multiple evaluation jobs to test concurrent processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:00:32.387539559Z",
     "start_time": "2025-12-15T10:00:32.323033071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåê External HTTP Request: POST http://localhost:8000/api/v1/evaluations\n",
      "üì° Response Status: 200\n",
      "üìã JSON Response: {\n",
      "  \"job_id\": \"dc5f5549-c75d-4865-acb3-7ceda6bba4b4\",\n",
      "  \"status\": \"pending\",\n",
      "  \"evaluation_status\": null,\n",
      "  \"request\": {\n",
      "    \"benchmark_id\": \"hellaswag\",\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt2\",\n",
      "      \"provider\": \"huggingface\",\n",
      "      \"parameters\": {},\n",
      "      \"device\": null,\n",
      "      \"batch_size\": null\n",
      "    },\n",
      "    \"num_examples\": 5,\n",
      "    \"num_few_shot\": null,\n",
      "    \"random_seed\": 42,\n",
      "    \"benchmark_config\": {},\n",
      "    \"experiment_name\": \"batch_evaluation_1\",\n",
      "    \"tags\": {},\n",
      "    \"priority\": 0\n",
      "  },\n",
      "  \"submitted_at\": \"2025-12-15T10:00:32.327101Z\",\n",
      "  \"started_at\": null,\n",
      "  \"completed_at\": null,\n",
      "  \"progress\": null,\n",
      "  \"current_step\": null,\n",
      "  \"total_steps\": null,\n",
      "  \"completed_steps\": null,\n",
      "  \"error_message\": null,\n",
      "  \"error_details\": null,\n",
      "  \"estimated_duration\": null,\n",
      "  \"actual_duration\": null\n",
      "}\n",
      "‚úÖ Submitted job 1: dc5f5549-c75d-4865-acb3-7ceda6bba4b4\n",
      "\n",
      "üåê External HTTP Request: POST http://localhost:8000/api/v1/evaluations\n",
      "üì° Response Status: 200\n",
      "üìã JSON Response: {\n",
      "  \"job_id\": \"2b5125fa-ee59-43d6-80e2-6208d2092684\",\n",
      "  \"status\": \"pending\",\n",
      "  \"evaluation_status\": null,\n",
      "  \"request\": {\n",
      "    \"benchmark_id\": \"arc:easy\",\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt2\",\n",
      "      \"provider\": \"huggingface\",\n",
      "      \"parameters\": {},\n",
      "      \"device\": null,\n",
      "      \"batch_size\": null\n",
      "    },\n",
      "    \"num_examples\": 5,\n",
      "    \"num_few_shot\": null,\n",
      "    \"random_seed\": 42,\n",
      "    \"benchmark_config\": {},\n",
      "    \"experiment_name\": \"batch_evaluation_2\",\n",
      "    \"tags\": {},\n",
      "    \"priority\": 0\n",
      "  },\n",
      "  \"submitted_at\": \"2025-12-15T10:00:32.328228Z\",\n",
      "  \"started_at\": null,\n",
      "  \"completed_at\": null,\n",
      "  \"progress\": null,\n",
      "  \"current_step\": null,\n",
      "  \"total_steps\": null,\n",
      "  \"completed_steps\": null,\n",
      "  \"error_message\": null,\n",
      "  \"error_details\": null,\n",
      "  \"estimated_duration\": null,\n",
      "  \"actual_duration\": null\n",
      "}\n",
      "‚úÖ Submitted job 2: 2b5125fa-ee59-43d6-80e2-6208d2092684\n",
      "\n",
      "üåê External HTTP Request: POST http://localhost:8000/api/v1/evaluations\n",
      "üì° Response Status: 200\n",
      "üìã JSON Response: {\n",
      "  \"job_id\": \"480b7961-b3ff-460c-8fe7-7032a570fae5\",\n",
      "  \"status\": \"pending\",\n",
      "  \"evaluation_status\": null,\n",
      "  \"request\": {\n",
      "    \"benchmark_id\": \"arc:challenge\",\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt2\",\n",
      "      \"provider\": \"huggingface\",\n",
      "      \"parameters\": {},\n",
      "      \"device\": null,\n",
      "      \"batch_size\": null\n",
      "    },\n",
      "    \"num_examples\": 5,\n",
      "    \"num_few_shot\": null,\n",
      "    \"random_seed\": 42,\n",
      "    \"benchmark_config\": {},\n",
      "    \"experiment_name\": \"batch_evaluation_3\",\n",
      "    \"tags\": {},\n",
      "    \"priority\": 0\n",
      "  },\n",
      "  \"submitted_at\": \"2025-12-15T10:00:32.329170Z\",\n",
      "  \"started_at\": null,\n",
      "  \"completed_at\": null,\n",
      "  \"progress\": null,\n",
      "  \"current_step\": null,\n",
      "  \"total_steps\": null,\n",
      "  \"completed_steps\": null,\n",
      "  \"error_message\": null,\n",
      "  \"error_details\": null,\n",
      "  \"estimated_duration\": null,\n",
      "  \"actual_duration\": null\n",
      "}\n",
      "‚úÖ Submitted job 3: 480b7961-b3ff-460c-8fe7-7032a570fae5\n",
      "\n",
      "Submitted 3 jobs for concurrent processing\n"
     ]
    }
   ],
   "source": [
    "# Submit multiple evaluation jobs\n",
    "if isinstance(benchmarks_response, list) and len(benchmarks_response) >= 2:\n",
    "    job_ids = []\n",
    "\n",
    "    for i, benchmark in enumerate(benchmarks_response[:3]):  # Test with up to 3 benchmarks\n",
    "        evaluation_request = {\n",
    "            \"benchmark_id\": benchmark['benchmark_id'],\n",
    "            \"model\": {\n",
    "                \"name\": \"gpt2\",\n",
    "                \"provider\": \"huggingface\"\n",
    "            },\n",
    "            \"num_examples\": 5,\n",
    "            \"experiment_name\": f\"batch_evaluation_{i+1}\"\n",
    "        }\n",
    "\n",
    "        job_response = make_request('POST', '/evaluations', evaluation_request)\n",
    "\n",
    "        if 'job_id' in job_response:\n",
    "            job_ids.append(job_response['job_id'])\n",
    "            print(f\"‚úÖ Submitted job {i+1}: {job_response['job_id']}\")\n",
    "\n",
    "    print(f\"\\nSubmitted {len(job_ids)} jobs for concurrent processing\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Not enough benchmarks available for batch testing\")\n",
    "    job_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:00:33.472079340Z",
     "start_time": "2025-12-15T10:00:33.410121792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking status of all submitted jobs:\n",
      "\n",
      "üåê External HTTP Request: GET http://localhost:8000/api/v1/evaluations/dc5f5549-c75d-4865-acb3-7ceda6bba4b4\n",
      "üì° Response Status: 200\n",
      "üìã JSON Response: {\n",
      "  \"job_id\": \"dc5f5549-c75d-4865-acb3-7ceda6bba4b4\",\n",
      "  \"status\": \"running\",\n",
      "  \"evaluation_status\": null,\n",
      "  \"request\": {\n",
      "    \"benchmark_id\": \"hellaswag\",\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt2\",\n",
      "      \"provider\": \"huggingface\",\n",
      "      \"parameters\": {},\n",
      "      \"device\": null,\n",
      "      \"batch_size\": null\n",
      "    },\n",
      "    \"num_examples\": 5,\n",
      "    \"num_few_shot\": null,\n",
      "    \"random_seed\": 42,\n",
      "    \"benchmark_config\": {},\n",
      "    \"experiment_name\": \"batch_evaluation_1\",\n",
      "    \"tags\": {},\n",
      "    \"priority\": 0\n",
      "  },\n",
      "  \"submitted_at\": \"2025-12-15T10:00:32.327101Z\",\n",
      "  \"started_at\": \"2025-12-15T10:00:32.327318Z\",\n",
      "  \"completed_at\": null,\n",
      "  \"progress\": 0.3,\n",
      "  \"current_step\": null,\n",
      "  \"total_steps\": null,\n",
      "  \"completed_steps\": null,\n",
      "  \"error_message\": null,\n",
      "  \"error_details\": null,\n",
      "  \"estimated_duration\": null,\n",
      "  \"actual_duration\": null\n",
      "}\n",
      "   Job dc5f5549-c75d-4865-acb3-7ceda6bba4b4: hellaswag - running (30.0%)\n",
      "\n",
      "üåê External HTTP Request: GET http://localhost:8000/api/v1/evaluations/2b5125fa-ee59-43d6-80e2-6208d2092684\n",
      "üì° Response Status: 200\n",
      "üìã JSON Response: {\n",
      "  \"job_id\": \"2b5125fa-ee59-43d6-80e2-6208d2092684\",\n",
      "  \"status\": \"running\",\n",
      "  \"evaluation_status\": null,\n",
      "  \"request\": {\n",
      "    \"benchmark_id\": \"arc:easy\",\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt2\",\n",
      "      \"provider\": \"huggingface\",\n",
      "      \"parameters\": {},\n",
      "      \"device\": null,\n",
      "      \"batch_size\": null\n",
      "    },\n",
      "    \"num_examples\": 5,\n",
      "    \"num_few_shot\": null,\n",
      "    \"random_seed\": 42,\n",
      "    \"benchmark_config\": {},\n",
      "    \"experiment_name\": \"batch_evaluation_2\",\n",
      "    \"tags\": {},\n",
      "    \"priority\": 0\n",
      "  },\n",
      "  \"submitted_at\": \"2025-12-15T10:00:32.328228Z\",\n",
      "  \"started_at\": \"2025-12-15T10:00:32.328380Z\",\n",
      "  \"completed_at\": null,\n",
      "  \"progress\": 0.3,\n",
      "  \"current_step\": null,\n",
      "  \"total_steps\": null,\n",
      "  \"completed_steps\": null,\n",
      "  \"error_message\": null,\n",
      "  \"error_details\": null,\n",
      "  \"estimated_duration\": null,\n",
      "  \"actual_duration\": null\n",
      "}\n",
      "   Job 2b5125fa-ee59-43d6-80e2-6208d2092684: arc:easy - running (30.0%)\n",
      "\n",
      "üåê External HTTP Request: GET http://localhost:8000/api/v1/evaluations/480b7961-b3ff-460c-8fe7-7032a570fae5\n",
      "üì° Response Status: 200\n",
      "üìã JSON Response: {\n",
      "  \"job_id\": \"480b7961-b3ff-460c-8fe7-7032a570fae5\",\n",
      "  \"status\": \"running\",\n",
      "  \"evaluation_status\": null,\n",
      "  \"request\": {\n",
      "    \"benchmark_id\": \"arc:challenge\",\n",
      "    \"model\": {\n",
      "      \"name\": \"gpt2\",\n",
      "      \"provider\": \"huggingface\",\n",
      "      \"parameters\": {},\n",
      "      \"device\": null,\n",
      "      \"batch_size\": null\n",
      "    },\n",
      "    \"num_examples\": 5,\n",
      "    \"num_few_shot\": null,\n",
      "    \"random_seed\": 42,\n",
      "    \"benchmark_config\": {},\n",
      "    \"experiment_name\": \"batch_evaluation_3\",\n",
      "    \"tags\": {},\n",
      "    \"priority\": 0\n",
      "  },\n",
      "  \"submitted_at\": \"2025-12-15T10:00:32.329170Z\",\n",
      "  \"started_at\": \"2025-12-15T10:00:32.329297Z\",\n",
      "  \"completed_at\": null,\n",
      "  \"progress\": 0.3,\n",
      "  \"current_step\": null,\n",
      "  \"total_steps\": null,\n",
      "  \"completed_steps\": null,\n",
      "  \"error_message\": null,\n",
      "  \"error_details\": null,\n",
      "  \"estimated_duration\": null,\n",
      "  \"actual_duration\": null\n",
      "}\n",
      "   Job 480b7961-b3ff-460c-8fe7-7032a570fae5: arc:challenge - running (30.0%)\n"
     ]
    }
   ],
   "source": [
    "# Check status of all jobs\n",
    "if job_ids:\n",
    "    print(\"\\nChecking status of all submitted jobs:\")\n",
    "\n",
    "    for job_id in job_ids:\n",
    "        status_response = make_request('GET', f'/evaluations/{job_id}')\n",
    "        if 'status' in status_response:\n",
    "            status = status_response['status']\n",
    "            benchmark_id = status_response['request']['benchmark_id']\n",
    "            progress = status_response.get('progress', 0)\n",
    "            print(f\"   Job {job_id}: {benchmark_id} - {status} ({progress*100 if progress else 0:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"   Job {job_id}: Failed to get status\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No batch jobs to check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Job Cancellation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Multiple Evaluations\n",
    "\n",
    "Let's submit multiple evaluation jobs to test concurrent processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:00:49.062073879Z",
     "start_time": "2025-12-15T10:00:49.000461010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing error handling with invalid benchmark ID:\n",
      "\n",
      "üåê External HTTP Request: POST http://localhost:8000/api/v1/evaluations\n",
      "üì° Response Status: 404\n",
      "üìã JSON Response: {\n",
      "  \"error_type\": \"NotFound\",\n",
      "  \"error_message\": \"Resource not found\",\n",
      "  \"path\": \"/api/v1/evaluations\"\n",
      "}\n",
      "Unexpected response ‚ùå\n"
     ]
    }
   ],
   "source": [
    "# Test with invalid benchmark ID\n",
    "print(\"Testing error handling with invalid benchmark ID:\")\n",
    "invalid_request = {\n",
    "    \"benchmark_id\": \"non_existent_benchmark\",\n",
    "    \"model\": {\n",
    "        \"name\": \"gpt2\"\n",
    "    },\n",
    "    \"num_examples\": 10\n",
    "}\n",
    "\n",
    "error_response = make_request('POST', '/evaluations', invalid_request)\n",
    "print(\"Expected error response received ‚úÖ\" if 'error' in error_response or 'detail' in error_response else \"Unexpected response ‚ùå\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Cleanup\n",
    "\n",
    "Stop and remove the container when we're done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:01:01.095194354Z",
     "start_time": "2025-12-15T10:01:01.037339145Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up...\n",
      "To stop and remove the container, run these commands in your terminal:\n",
      "   podman stop lighteval-adapter\n",
      "   podman rm lighteval-adapter\n",
      "   podman rmi evalhub/lighteval-adapter:latest  # Optional: remove image\n"
     ]
    }
   ],
   "source": [
    "# Stop and remove the container\n",
    "print(\"Cleaning up...\")\n",
    "print(\"To stop and remove the container, run these commands in your terminal:\")\n",
    "print(f\"   podman stop {CONTAINER_NAME}\")\n",
    "print(f\"   podman rm {CONTAINER_NAME}\")\n",
    "print(f\"   podman rmi {FULL_IMAGE_NAME}  # Optional: remove image\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
